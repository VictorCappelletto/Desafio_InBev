# Deploy em Produ√ß√£o

Guia completo para deploy do projeto em ambientes produtivos.

---

## üéØ Ambientes Suportados

O projeto pode ser deployado em:

### 1. **Astronomer Cloud** (Recomendado)
- ‚úÖ **Vantagens:** Gerenciamento simplificado, auto-scaling, monitoramento integrado
- üìù **Custo:** Plano pago ($0.25/AU/hora)
- üîó **Docs:** [Astronomer Deploy Guide](https://docs.astronomer.io/astro/deploy-code)

### 2. **Azure Container Instances**
- ‚úÖ **Vantagens:** Integra√ß√£o nativa com Azure, baixo custo
- üìù **Setup:** Requer configura√ß√£o de networking e secrets

### 3. **Kubernetes (GKE/EKS/AKS)**
- ‚úÖ **Vantagens:** Total controle, customiza√ß√£o ilimitada
- ‚ö†Ô∏è **Complexidade:** Requer expertise em K8s

---

## üöÄ Deploy via Astronomer Cloud

### Passo 1: Criar Conta
```bash
# Criar conta gratuita (trial)
https://www.astronomer.io/try-astro/

# Instalar Astro CLI (se ainda n√£o tiver)
brew install astro
```

### Passo 2: Login e Deploy
```bash
# Login
astro login

# Deploy para workspace
astro deploy

# Escolher workspace e cluster
# Aguardar build (~5 min)
```

### Passo 3: Configurar Secrets
```bash
# Via Astro UI:
# 1. Settings ‚Üí Environment Variables
# 2. Adicionar secrets:
AZURE_SQL_PASSWORD=***
DATABRICKS_TOKEN=***
DATABRICKS_HOST=https://...
ADF_TENANT_ID=***
ADF_CLIENT_ID=***
ADF_CLIENT_SECRET=***
```

### Passo 4: Verificar Deploy
```bash
# Ver logs
astro deployment logs

# Ver status
astro deployment list

# URL da UI: https://your-deployment.astro.datakin.com
```

---

## üê≥ Deploy via Docker Compose

### Passo 1: Build da Imagem
```bash
# Build otimizado
docker build -t desafio-inbev:latest .

# Push para registry (opcional)
docker tag desafio-inbev:latest your-registry/desafio-inbev:latest
docker push your-registry/desafio-inbev:latest
```

### Passo 2: Criar `docker-compose.prod.yml`
```yaml
version: '3'
services:
  webserver:
    image: desafio-inbev:latest
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://...
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AZURE_SQL_PASSWORD: ${AZURE_SQL_PASSWORD}
      DATABRICKS_TOKEN: ${DATABRICKS_TOKEN}
    ports:
      - "8080:8080"
  
  scheduler:
    image: desafio-inbev:latest
    command: scheduler
    depends_on:
      - postgres
      - redis
  
  worker:
    image: desafio-inbev:latest
    command: celery worker
    depends_on:
      - postgres
      - redis
  
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
  
  redis:
    image: redis:7
```

### Passo 3: Deploy
```bash
# Start
docker-compose -f docker-compose.prod.yml up -d

# Ver logs
docker-compose logs -f

# Stop
docker-compose -f docker-compose.prod.yml down
```

---

## ‚ò∏Ô∏è Deploy via Kubernetes

### Passo 1: Helm Chart (Airflow)
```bash
# Adicionar repo
helm repo add apache-airflow https://airflow.apache.org
helm repo update

# Instalar
helm install airflow apache-airflow/airflow \
  --namespace airflow \
  --create-namespace \
  --set images.airflow.repository=your-registry/desafio-inbev \
  --set images.airflow.tag=latest
```

### Passo 2: Configurar Secrets
```bash
# Criar secret
kubectl create secret generic airflow-secrets \
  --from-literal=AZURE_SQL_PASSWORD='***' \
  --from-literal=DATABRICKS_TOKEN='***' \
  --namespace airflow
```

### Passo 3: Deploy via Kubectl
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-webserver
  namespace: airflow
spec:
  replicas: 2
  selector:
    matchLabels:
      app: airflow-webserver
  template:
    metadata:
      labels:
        app: airflow-webserver
    spec:
      containers:
      - name: webserver
        image: your-registry/desafio-inbev:latest
        ports:
        - containerPort: 8080
        envFrom:
        - secretRef:
            name: airflow-secrets
```

---

## üîí Gerenciamento de Secrets

### Op√ß√£o 1: Azure Key Vault (Recomendado)
```python
# dags/config/azure_secrets.py
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient

credential = DefaultAzureCredential()
client = SecretClient(vault_url="https://your-vault.vault.azure.net/", credential=credential)

# Usar em DAGs:
AZURE_SQL_PASSWORD = client.get_secret("azure-sql-password").value
```

### Op√ß√£o 2: Airflow Variables (Encrypted)
```bash
# Via CLI:
airflow variables set AZURE_SQL_PASSWORD "***" --json

# Via UI:
# Admin ‚Üí Variables ‚Üí + ‚Üí Key/Value
```

### Op√ß√£o 3: Environment Variables
```bash
# .env (n√£o commitar!)
export AZURE_SQL_PASSWORD='***'
export DATABRICKS_TOKEN='***'
```

---

## üìä Monitoramento

### 1. **Airflow UI**
- üìà DAG runs history
- ‚è±Ô∏è Task duration
- ‚ùå Failure logs

### 2. **Azure Monitor** (se usando Azure)
```bash
# Habilitar logs
az monitor diagnostic-settings create \
  --resource /subscriptions/.../airflow \
  --logs '[{"category": "AirflowLogs", "enabled": true}]'
```

### 3. **Prometheus + Grafana**
```yaml
# metrics.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-metrics
data:
  statsd_exporter.yml: |
    mappings:
    - match: "airflow.dag.*.*.duration"
      name: "airflow_dag_duration"
```

---

## üîÑ CI/CD Automation

### GitHub Actions Deploy (exemplo)
```yaml
# .github/workflows/cd.yml
name: Deploy to Production

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Deploy to Astronomer
        run: |
          astro login --token ${{ secrets.ASTRO_TOKEN }}
          astro deploy <deployment-id>
```

---

## ‚úÖ Checklist de Deploy

Antes de fazer deploy para produ√ß√£o:

- [ ] ‚úÖ Todos os testes passando (`poetry run task test`)
- [ ] ‚úÖ Lint sem erros (`poetry run task lint`)
- [ ] ‚úÖ Security scan limpo (CI)
- [ ] ‚úÖ Secrets configurados (n√£o hardcoded)
- [ ] ‚úÖ Connections testadas (Azure SQL, Databricks, ADF)
- [ ] ‚úÖ Vari√°veis de ambiente configuradas
- [ ] ‚úÖ Logs configurados (n√≠vel INFO ou WARNING)
- [ ] ‚úÖ Alertas configurados (email/Slack)
- [ ] ‚úÖ Backup do metastore configurado
- [ ] ‚úÖ Monitoramento configurado
- [ ] ‚úÖ Documenta√ß√£o atualizada

---

## üÜò Rollback

Se algo der errado:

### Astronomer
```bash
# Listar deploys
astro deployment list

# Rollback para vers√£o anterior
astro deployment rollback <deployment-id>
```

### Docker
```bash
# Stop current
docker-compose down

# Deploy vers√£o anterior
docker-compose -f docker-compose.prod.yml up -d
```

### Kubernetes
```bash
# Rollback deployment
kubectl rollout undo deployment/airflow-webserver -n airflow

# Ver hist√≥rico
kubectl rollout history deployment/airflow-webserver -n airflow
```

---

## üìù Suporte

Para d√∫vidas sobre deploy:

- üìñ [Astronomer Docs](https://docs.astronomer.io/)
- üìñ [Airflow Docs - Production](https://airflow.apache.org/docs/apache-airflow/stable/production-deployment.html)
- üìñ [Azure AKS Docs](https://learn.microsoft.com/en-us/azure/aks/)

---

## üéØ Pr√≥ximos Passos

Ap√≥s deploy em produ√ß√£o:

1. **Habilitar auto-scaling** (Astronomer/K8s)
2. **Configurar alertas** (PagerDuty, Slack)
3. **Implementar observability completa** (framework j√° criado)
4. **Configurar backup autom√°tico** do metastore
5. **Documentar runbooks** para incidentes comuns
