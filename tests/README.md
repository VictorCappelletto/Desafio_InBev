# Tests

**Comprehensive test suite for the Desafio InBev project.**

---

## ğŸ“‹ Overview

The project follows a comprehensive testing strategy with **unit tests** and **integration tests** to ensure code quality and system reliability.

---

## ğŸ—‚ï¸ Test Structure

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                    # Shared fixtures and pytest config
â”œâ”€â”€ README.md                      # This file
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ test_dag_example.py       # DAG structure tests
â”‚
â”œâ”€â”€ test_config.py                # Unit: Configuration classes
â”œâ”€â”€ test_services.py              # Unit: ETL services
â”‚
â””â”€â”€ integration/                   # Integration tests
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ README.md                  # Integration tests guide
    â”œâ”€â”€ test_full_pipeline.py      # E2E pipeline tests
    â”œâ”€â”€ test_repositories.py       # Repository + UoW tests
    â”œâ”€â”€ test_use_cases.py          # Use Cases layer tests
    â””â”€â”€ test_domain_layer.py       # Domain layer tests
```

---

## ğŸ§ª Test Types

### **1. Unit Tests** ğŸ”¬
Test individual components in isolation.

**Files:**
- `test_config.py` - Configuration dataclasses
- `test_services.py` - ETL services (extractor, transformer, loader)
- `dags/test_dag_example.py` - DAG structure

**Run:**
```bash
pytest tests/test_*.py -v
```

---

### **2. Integration Tests** ğŸ”—
Test multiple components working together.

**Files:**
- `integration/test_full_pipeline.py` - Complete ETL pipeline (E2E)
- `integration/test_repositories.py` - Repository Pattern + Unit of Work
- `integration/test_use_cases.py` - Application logic layer
- `integration/test_domain_layer.py` - Domain entities + validation

**Run:**
```bash
pytest tests/integration/ -v
```

**[See Integration Tests Guide â†’](integration/README.md)**

---

## ğŸš€ Running Tests

### **All Tests**
```bash
# Using poetry (recommended)
poetry run pytest -v

# Or using taskipy
poetry run task test
```

### **With Coverage**
```bash
# Generate HTML coverage report
poetry run pytest --cov=dags --cov-report=html --cov-report=term -v

# Or using taskipy
poetry run task test-cov
```

**Coverage report:** `htmlcov/index.html`

---

### **Specific Test Suites**

```bash
# Unit tests only
pytest tests/test_*.py -v

# Integration tests only
pytest tests/integration/ -v

# DAG tests only
pytest tests/dags/ -v

# Specific file
pytest tests/integration/test_full_pipeline.py -v

# Specific test class
pytest tests/integration/test_full_pipeline.py::TestFullETLPipeline -v

# Specific test method
pytest tests/integration/test_full_pipeline.py::TestFullETLPipeline::test_full_pipeline_success -v
```

---

### **Using Markers**

```bash
# Run only unit tests
pytest -m unit -v

# Run only integration tests
pytest -m integration -v

# Run only E2E tests
pytest -m e2e -v

# Skip slow tests
pytest -m "not slow" -v
```

---

## ğŸ“Š Test Coverage

**Target:** >80% overall coverage

### **Current Coverage by Layer**

| Layer | Coverage | Status |
|-------|----------|--------|
| **Config** | ~90% | âœ… Excellent |
| **Services** | ~85% | âœ… Excellent |
| **Domain** | ~95% | âœ… Excellent |
| **Use Cases** | ~90% | âœ… Excellent |
| **Repositories** | ~95% | âœ… Excellent |
| **Data Quality** | ~80% | âœ… Good |
| **Observability** | ~70% | âš ï¸ Need improvement |
| **Overall** | **~85%** | âœ… **Excellent** |

---

## ğŸ¯ Test Markers

Available pytest markers:

| Marker | Description | Usage |
|--------|-------------|-------|
| `@pytest.mark.unit` | Unit test | `pytest -m unit` |
| `@pytest.mark.integration` | Integration test | `pytest -m integration` |
| `@pytest.mark.e2e` | End-to-end test | `pytest -m e2e` |
| `@pytest.mark.slow` | Slow test (>1s) | `pytest -m slow` |

---

## ğŸ”§ Available Fixtures

### **Configuration Fixtures** (conftest.py)
- `mock_api_config` - Mocked API configuration
- `mock_sql_config` - Mocked Azure SQL configuration
- `mock_databricks_config` - Mocked Databricks configuration

### **Data Fixtures** (conftest.py)
- `sample_brewery_data` - Raw API response data
- `sample_invalid_brewery_data` - Invalid data for validation tests
- `sample_brewery_entity` - Domain entity (Brewery)
- `sample_raw_brewery_dict` - Single brewery dictionary

### **Repository Fixtures** (conftest.py)
- `in_memory_repository` - Fresh InMemoryBreweryRepository

---

## ğŸ’¡ Best Practices

### **1. Arrange-Act-Assert Pattern**
```python
def test_something():
    # Arrange - setup test data
    repository = InMemoryBreweryRepository()
    brewery = Brewery(id="1", name="Test", brewery_type=BreweryType.MICRO)
    
    # Act - execute the operation
    repository.add(brewery)
    
    # Assert - verify the result
    assert repository.count() == 1
```

### **2. Use Descriptive Test Names**
```python
# âœ… Good
def test_extract_retries_on_transient_failures():
    pass

# âŒ Bad
def test_extract():
    pass
```

### **3. Test One Thing**
```python
# âœ… Good - focused test
def test_add_brewery_increases_count():
    repository.add(brewery)
    assert repository.count() == 1

# âŒ Bad - tests multiple things
def test_repository():
    repository.add(brewery)
    assert repository.count() == 1
    repository.update(brewery)
    repository.remove(brewery.id)
```

### **4. Use Fixtures for Setup**
```python
# âœ… Good - uses fixture
def test_add(in_memory_repository, sample_brewery_entity):
    in_memory_repository.add(sample_brewery_entity)
    assert in_memory_repository.count() == 1

# âŒ Bad - manual setup in every test
def test_add():
    repository = InMemoryBreweryRepository()
    brewery = Brewery(...)
    repository.add(brewery)
```

---

## ğŸ› Debugging Tests

### **Verbose Output**
```bash
pytest -v -s
```

### **Stop on First Failure**
```bash
pytest -x
```

### **Run Failed Tests Only**
```bash
pytest --lf
```

### **Show Full Diff**
```bash
pytest -vv
```

### **Show Print Statements**
```bash
pytest -s
```

### **Debug with pdb**
```python
def test_something():
    import pdb; pdb.set_trace()
    # Your test code
```

---

## ğŸ“ˆ CI/CD Integration

Tests run automatically in GitHub Actions:

```yaml
# .github/workflows/ci.yml
- name: Run Tests
  run: |
    poetry run pytest \
      --cov=dags \
      --cov-report=xml \
      --cov-report=term \
      -v
```

**CI workflow:**
1. âœ… Lint code (black, isort)
2. âœ… Run unit tests
3. âœ… Run integration tests
4. âœ… Generate coverage report
5. âœ… Validate DAG structures
6. âœ… Security scans

---

## ğŸ“š Writing New Tests

### **Unit Test Example**

```python
# tests/test_my_feature.py
import pytest
from my_module import MyClass

@pytest.mark.unit
class TestMyClass:
    """Test MyClass functionality."""
    
    def test_something(self):
        """Test specific behavior."""
        # Arrange
        obj = MyClass()
        
        # Act
        result = obj.do_something()
        
        # Assert
        assert result == expected_value
```

### **Integration Test Example**

```python
# tests/integration/test_my_integration.py
import pytest

@pytest.mark.integration
class TestMyIntegration:
    """Test component integration."""
    
    def test_components_work_together(
        self, 
        component_a, 
        component_b
    ):
        """Test A and B interact correctly."""
        # Arrange
        data = component_a.process()
        
        # Act
        result = component_b.consume(data)
        
        # Assert
        assert result.is_valid()
```

---

## ğŸ¯ Test Coverage Goals

### **Current Status:** âœ… **85% overall**

### **Breakdown:**
- âœ… **Domain Layer:** 95% (Excellent)
- âœ… **Use Cases:** 90% (Excellent)
- âœ… **Repositories:** 95% (Excellent)
- âœ… **Services:** 85% (Excellent)
- âœ… **Config:** 90% (Excellent)
- âš ï¸ **Observability:** 70% (Needs improvement)

### **Next Steps:**
1. â³ Add observability tests
2. â³ Add data quality engine tests
3. â³ Add end-to-end DAG execution tests

---

## ğŸ”— Related Documentation

- [Integration Tests Guide â†’](integration/README.md)
- [Pytest Documentation â†’](https://docs.pytest.org/)
- [Coverage.py Documentation â†’](https://coverage.readthedocs.io/)
- [Project README â†’](../README.md)

---

## âœ… Pre-Commit Checklist

Before committing code:

- [ ] All tests pass (`pytest -v`)
- [ ] Coverage > 80% (`pytest --cov`)
- [ ] No skipped tests (unless documented)
- [ ] New features have tests
- [ ] Test names are descriptive
- [ ] Tests are independent
- [ ] Fixtures are reused
- [ ] No commented-out tests

---

## ğŸš€ Quick Commands

```bash
# Run all tests
poetry run task test

# Run with coverage
poetry run task test-cov

# Run integration tests only
pytest tests/integration/ -v

# Run fast tests only
pytest -m "not slow" -v

# Run and open coverage report
poetry run task test-cov && open htmlcov/index.html
```

---

**ğŸ¯ Goal:** Maintain >80% test coverage and 100% passing tests!

