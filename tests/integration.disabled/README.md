# Integration Tests

**Comprehensive integration tests covering the complete system.**

---

## ğŸ“‹ Overview

Integration tests verify that multiple components work together correctly. Unlike unit tests (which test components in isolation), integration tests ensure proper interaction between layers.

---

## ğŸ—‚ï¸ Test Structure

```
tests/integration/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_full_pipeline.py      # E2E pipeline tests
â”œâ”€â”€ test_repositories.py        # Repository + Unit of Work tests
â”œâ”€â”€ test_use_cases.py          # Use Cases layer tests
â”œâ”€â”€ test_domain_layer.py       # Domain entities + validation tests
â””â”€â”€ README.md                  # This file
```

---

## ğŸ§ª Test Suites

### **1. test_full_pipeline.py** (E2E)
**Purpose:** End-to-end pipeline testing

**Tests:**
- âœ… Complete ETL pipeline (Extract â†’ Transform â†’ Load â†’ Validate)
- âœ… Extraction failures handling
- âœ… Invalid data handling
- âœ… Quality filtering
- âœ… Duplicate handling
- âœ… Real API extractor (mocked requests)
- âœ… Metrics collection

**Run:**
```bash
pytest tests/integration/test_full_pipeline.py -v
```

---

### **2. test_repositories.py**
**Purpose:** Repository Pattern + Unit of Work

**Tests:**
- âœ… CRUD operations (Create, Read, Update, Delete)
- âœ… Batch operations (add_many)
- âœ… Query operations (find_by_type, find_by_location)
- âœ… Duplicate detection
- âœ… Unit of Work commit
- âœ… Unit of Work rollback on error
- âœ… Context manager support

**Run:**
```bash
pytest tests/integration/test_repositories.py -v
```

---

### **3. test_use_cases.py**
**Purpose:** Application Logic (Use Cases Layer)

**Tests:**
- âœ… ExtractBreweriesUseCase (with retry logic)
- âœ… TransformBreweriesUseCase (validation + skipping)
- âœ… LoadBreweriesUseCase (quality filtering)
- âœ… ValidateBreweriesQualityUseCase (strict mode)
- âœ… Error handling
- âœ… Empty input handling
- âœ… Metrics collection

**Run:**
```bash
pytest tests/integration/test_use_cases.py -v
```

---

### **4. test_domain_layer.py**
**Purpose:** Domain-Driven Design (DDD)

**Tests:**
- âœ… Value Objects (Coordinates, Address, Location, Contact)
- âœ… Immutability enforcement
- âœ… Validation (coordinates, names, types)
- âœ… Brewery entity creation
- âœ… Factory methods (from_dict)
- âœ… Aggregate quality scoring
- âœ… Domain validators

**Run:**
```bash
pytest tests/integration/test_domain_layer.py -v
```

---

## ğŸš€ Running Tests

### **All Integration Tests**
```bash
pytest tests/integration/ -v
```

### **With Coverage**
```bash
pytest tests/integration/ --cov=dags --cov-report=html --cov-report=term
```

### **Specific Test Class**
```bash
pytest tests/integration/test_full_pipeline.py::TestFullETLPipeline -v
```

### **Specific Test Method**
```bash
pytest tests/integration/test_full_pipeline.py::TestFullETLPipeline::test_full_pipeline_success -v
```

### **With Markers**
```bash
# Run only integration tests
pytest -m integration -v

# Run E2E tests
pytest -m e2e -v

# Skip slow tests
pytest -m "not slow" -v
```

---

## ğŸ“Š Test Coverage

**Target:** >80% coverage

**Current Layers:**
- âœ… **E2E Pipeline:** Complete flow coverage
- âœ… **Repositories:** CRUD + UoW coverage
- âœ… **Use Cases:** All 4 use cases covered
- âœ… **Domain:** Entities + VOs + Validators covered

---

## ğŸ¯ Test Markers

Tests can be marked with pytest markers:

```python
@pytest.mark.integration
def test_something():
    """Integration test."""
    pass

@pytest.mark.e2e
def test_full_flow():
    """End-to-end test."""
    pass

@pytest.mark.slow
def test_expensive():
    """Slow test (>1s)."""
    pass
```

**Run tests by marker:**
```bash
pytest -m integration  # Only integration tests
pytest -m e2e          # Only E2E tests
pytest -m "not slow"   # Skip slow tests
```

---

## ğŸ”§ Fixtures Available

### **From conftest.py:**
- `sample_brewery_data` - Raw API data
- `sample_brewery_entity` - Domain entity
- `sample_raw_brewery_dict` - Single brewery dict
- `in_memory_repository` - Fresh repository
- `mock_api_config` - Mocked API config
- `mock_sql_config` - Mocked SQL config
- `mock_databricks_config` - Mocked Databricks config

### **Usage:**
```python
def test_something(sample_brewery_entity, in_memory_repository):
    """Test using fixtures."""
    in_memory_repository.add(sample_brewery_entity)
    assert in_memory_repository.count() == 1
```

---

## ğŸ’¡ Best Practices

### **1. Test Independence**
Each test should be independent and not rely on other tests:

```python
# âœ… Good - uses fixture for fresh state
def test_add(in_memory_repository):
    in_memory_repository.add(brewery)
    assert in_memory_repository.count() == 1

# âŒ Bad - relies on previous test
def test_get():
    brewery = repository.get_by_id("1")  # Where did this come from?
```

### **2. Clear Test Names**
```python
# âœ… Good - describes what is tested
def test_pipeline_with_duplicate_handling():
    pass

# âŒ Bad - vague
def test_pipeline():
    pass
```

### **3. Arrange-Act-Assert Pattern**
```python
def test_something():
    # Arrange - setup
    repository = InMemoryBreweryRepository()
    brewery = Brewery(...)
    
    # Act - execute
    repository.add(brewery)
    
    # Assert - verify
    assert repository.count() == 1
```

### **4. Test One Thing**
```python
# âœ… Good - tests one behavior
def test_add_brewery():
    repository.add(brewery)
    assert repository.count() == 1

# âŒ Bad - tests multiple behaviors
def test_repository():
    repository.add(brewery)
    assert repository.count() == 1
    repository.update(brewery)
    assert brewery.name == "Updated"
    repository.remove(brewery.id)
    assert repository.count() == 0
```

---

## ğŸ› Debugging Tests

### **Run with verbose output:**
```bash
pytest tests/integration/ -v -s
```

### **Stop on first failure:**
```bash
pytest tests/integration/ -x
```

### **Run failed tests only:**
```bash
pytest tests/integration/ --lf
```

### **Show print statements:**
```bash
pytest tests/integration/ -s
```

### **Show full diff:**
```bash
pytest tests/integration/ -vv
```

---

## ğŸ“ˆ CI/CD Integration

Tests run automatically in CI/CD:

```yaml
# .github/workflows/ci.yml
- name: Run Integration Tests
  run: |
    pytest tests/integration/ \
      --cov=dags \
      --cov-report=xml \
      --cov-report=term \
      -v
```

---

## ğŸ”— Related

- [Unit Tests â†’](../test_config.py)
- [Conftest â†’](../conftest.py)
- [Domain Layer â†’](../../dags/domain/README.md)
- [Use Cases â†’](../../dags/use_cases/__init__.py)
- [Repositories â†’](../../dags/repositories/README.md)

---

## âœ… Checklist

Before committing:
- [ ] All integration tests pass
- [ ] Coverage > 80%
- [ ] No skipped tests (unless documented)
- [ ] Test names are descriptive
- [ ] Fixtures are used appropriately
- [ ] Tests are independent
- [ ] One assertion per test (preferably)

---

**Run all tests:**
```bash
poetry run task test
```

**With coverage:**
```bash
poetry run task test-cov
```

